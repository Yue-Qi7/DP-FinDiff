{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# import scikit-learn preprocessing functions\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "\n",
    "# import pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# import synthetic data vault libraries\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import sdv.evaluation.single_table as sdv_st\n",
    "\n",
    "# import utility libraries\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "\n",
    "# import visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init and set experiment parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 1234\n",
    "\n",
    "# set dimension of categorical embeddings\n",
    "cat_emb_dim = 2\n",
    "\n",
    "# set number of neurons per layer\n",
    "mlp_layers = [1024, 1024, 1024, 1024]\n",
    "\n",
    "# set non-linear activation function\n",
    "activation = 'lrelu'\n",
    "\n",
    "# set number of diffusion steps\n",
    "diffusion_steps = 500\n",
    "\n",
    "# set diffusion start and end betas\n",
    "diffusion_beta_start = 1e-4\n",
    "diffusion_beta_end = 0.02\n",
    "\n",
    "# set diffusion scheduler\n",
    "scheduler = 'linear'\n",
    "\n",
    "# set number of training epochs\n",
    "epochs = 30\n",
    "\n",
    "# set training batch size\n",
    "batch_size = 512\n",
    "\n",
    "# set training learning rate\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# set pytorch seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda seed\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, pre-process, and init the UCU Credit Card dataset\n",
    "The dataset is available under https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_url = 'https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip'\n",
    "\n",
    "# download the file\n",
    "response = requests.get(data_url)\n",
    "\n",
    "# determine the zip file\n",
    "zip_file = ZipFile(BytesIO(response.content))\n",
    "\n",
    "# extract the zip file\n",
    "zip_file.extractall('data')\n",
    "\n",
    "# read the UCI credit card dataset\n",
    "train_raw = pd.read_excel('data/default of credit card clients.xls', skiprows=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top 10 rows and attribute names of the dataset retreived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19394</td>\n",
       "      <td>19619</td>\n",
       "      <td>20024</td>\n",
       "      <td>2500</td>\n",
       "      <td>1815</td>\n",
       "      <td>657</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>542653</td>\n",
       "      <td>483003</td>\n",
       "      <td>473944</td>\n",
       "      <td>55000</td>\n",
       "      <td>40000</td>\n",
       "      <td>38000</td>\n",
       "      <td>20239</td>\n",
       "      <td>13750</td>\n",
       "      <td>13770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>221</td>\n",
       "      <td>-159</td>\n",
       "      <td>567</td>\n",
       "      <td>380</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>581</td>\n",
       "      <td>1687</td>\n",
       "      <td>1542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>140000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12211</td>\n",
       "      <td>11793</td>\n",
       "      <td>3719</td>\n",
       "      <td>3329</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13007</td>\n",
       "      <td>13912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13007</td>\n",
       "      <td>1122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "5   6      50000    1          1         2   37      0      0      0      0   \n",
       "6   7     500000    1          1         2   29      0      0      0      0   \n",
       "7   8     100000    2          2         2   23      0     -1     -1      0   \n",
       "8   9     140000    2          3         1   28      0      0      2      0   \n",
       "9  10      20000    1          3         2   35     -2     -2     -2     -2   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "5  ...      19394      19619      20024      2500      1815       657   \n",
       "6  ...     542653     483003     473944     55000     40000     38000   \n",
       "7  ...        221       -159        567       380       601         0   \n",
       "8  ...      12211      11793       3719      3329         0       432   \n",
       "9  ...          0      13007      13912         0         0         0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "5      1000      1000       800                           0  \n",
       "6     20239     13750     13770                           0  \n",
       "7       581      1687      1542                           0  \n",
       "8      1000      1000      1000                           0  \n",
       "9     13007      1122         0                           0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top 10 rows\n",
    "train_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default payment next month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display attribute names \n",
    "train_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set numerical and categorical dataset attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine categorical attributes\n",
    "cat_attrs = ['SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY0', 'PAY2', 'PAY3', 'PAY4', 'PAY5', 'PAY6']\n",
    "\n",
    "# determine numerical attributes\n",
    "num_attrs = ['LIMITBAL', 'BILLAMT1', 'BILLAMT2', 'BILLAMT3', 'BILLAMT4', 'BILLAMT5', 'BILLAMT6',\n",
    "             'PAYAMT1', 'PAYAMT2', 'PAYAMT3', 'PAYAMT4', 'PAYAMT5', 'PAYAMT6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process dataset attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove underscore in column names for correct inverse decoding\n",
    "train_raw.columns = [col.replace('_', '') for col in train_raw.columns]\n",
    "\n",
    "# convert categorical attributes to string\n",
    "train_raw[cat_attrs] = train_raw[cat_attrs].astype(str)\n",
    "\n",
    "# iterate over categorical attributes\n",
    "for cat_attr in cat_attrs:\n",
    "\n",
    "    # add col name to every categorical entry to make them distinguishable for embedding\n",
    "    train_raw[cat_attr] = cat_attr + '_' + train_raw[cat_attr].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dataset label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label\n",
    "label = train_raw['default payment next month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge selected categorical and numerical attributes\n",
    "train = train_raw[[*cat_attrs, *num_attrs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the quantile transformation\n",
    "num_scaler = QuantileTransformer(output_distribution='normal', random_state=seed)\n",
    "\n",
    "# fit transformation to numerical attributes\n",
    "num_scaler.fit(train[num_attrs])\n",
    "\n",
    "# transform numerical attributes\n",
    "train_num_scaled = num_scaler.transform(train[num_attrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary of categorical attributes\n",
    "vocabulary_classes = np.unique(train[cat_attrs])\n",
    "\n",
    "# init categorical attribute encoder \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit encoder to categorical attributes\n",
    "label_encoder.fit(vocabulary_classes)\n",
    "\n",
    "# transform categorical attributes\n",
    "train_cat_scaled = train[cat_attrs].apply(label_encoder.transform)\n",
    "\n",
    "# collect unique values of each categorical attribute\n",
    "vocab_per_attr = {cat_attr: set(train_cat_scaled[cat_attr]) for cat_attr in cat_attrs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numerical and categorical attributes as well as the labels to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical attributes\n",
    "train_num_torch = torch.FloatTensor(train_num_scaled)\n",
    "\n",
    "# convert categorical attributes\n",
    "train_cat_torch = torch.LongTensor(train_cat_scaled.values)\n",
    "\n",
    "# convert label\n",
    "label_torch = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to tensor dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tensor dataset\n",
    "train_set = TensorDataset(\n",
    "    train_cat_torch, # categorical attributes\n",
    "    train_num_torch, # numerical attributes\n",
    "    label_torch # dataset labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the data loader\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_set, # training dataset\n",
    "    batch_size=batch_size, # training batch size\n",
    "    num_workers=0, # number of workers\n",
    "    shuffle=True # shuffle training data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the FinDiff model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the FinDiff backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base feedforward network\n",
    "class BaseNetwork(nn.Module):\n",
    "\n",
    "    # define base network constructor\n",
    "    def __init__(self, hidden_size, activation='lrelu'):\n",
    "\n",
    "        # call super calass constructor \n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "        # init \n",
    "        self.layers = self.init_layers(hidden_size)\n",
    "\n",
    "        # case: lrelu activation\n",
    "        if activation == 'lrelu':\n",
    "\n",
    "            # set lrelu activation\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # case: relu activation\n",
    "        elif activation == 'relu':\n",
    "\n",
    "            # set relu activation\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        # case: tanh activation\n",
    "        elif activation == 'tanh':\n",
    "\n",
    "            # set tanh activation\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        # case: sigmoid activation\n",
    "        else:\n",
    "\n",
    "            # set sigmoid activation\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # define layer initialization \n",
    "    def init_layers(self, layer_dimensions):\n",
    "\n",
    "        # init layers\n",
    "        layers = []\n",
    "\n",
    "        # iterate over layer dimensions \n",
    "        for i in range(len(layer_dimensions)-1):\n",
    "\n",
    "            # init linear layer \n",
    "            layer = nn.Linear(layer_dimensions[i], layer_dimensions[i + 1], bias=True)\n",
    "            \n",
    "            # init linear layer weights\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            \n",
    "            # init linear layer bias\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "            # collecet linear layer \n",
    "            layers.append(layer)\n",
    "            \n",
    "            # register linear layer parameters\n",
    "            self.add_module('linear_' + str(i), layer)\n",
    "\n",
    "        # return layers\n",
    "        return layers\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # iterate over layers\n",
    "        for i in range(len(self.layers)):\n",
    "\n",
    "            # run layer forward pass \n",
    "            x = self.activation(self.layers[i](x))\n",
    "\n",
    "        # return forward pass result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the FinDiff model synthesizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP synthesizer network\n",
    "class MLPSynthesizer(nn.Module):\n",
    "\n",
    "    # define MLP synthesizer network constructor\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_in: int, \n",
    "            hidden_layers: list, \n",
    "            activation: str='lrelu', # layer activation \n",
    "            dim_t: int=64, \n",
    "            n_cat_tokens=None, # number of categorical tokens\n",
    "            n_cat_emb=None, # number of categorical dimensions\n",
    "            embedding=None, \n",
    "            embedding_learned=True, \n",
    "            n_classes=None\n",
    "        ):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(MLPSynthesizer, self).__init__()\n",
    "\n",
    "        # init ??? \n",
    "        self.dim_t = dim_t\n",
    "\n",
    "        # init synthesizer base feed forward network\n",
    "        self.backbone = BaseNetwork([dim_t, *hidden_layers], activation=activation)\n",
    "        \n",
    "        # case: categorical embedding defined\n",
    "        if embedding is not None:\n",
    "\n",
    "            # init pretrained embedding layer \n",
    "            self.cat_embedding = nn.Embedding.from_pretrained(embeddings=embedding)\n",
    "\n",
    "        # case: categorical embedding undefined \n",
    "        else:\n",
    "\n",
    "            # init new categorical embedding layer \n",
    "            self.cat_embedding = nn.Embedding(n_cat_tokens, n_cat_emb, max_norm=None, scale_grad_by_freq=False)\n",
    "\n",
    "            # activate categorical embedding layer learning\n",
    "            self.cat_embedding.weight.requires_grad = embedding_learned\n",
    "\n",
    "        # case: data classes available\n",
    "        if n_classes is not None:\n",
    "\n",
    "            # init label embedding layer \n",
    "            self.label_embedding = nn.Embedding(n_classes, dim_t)\n",
    "\n",
    "        # define input data projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(d_in, dim_t), # linear layer \n",
    "            nn.SiLU(), # silu activation\n",
    "            nn.Linear(dim_t, dim_t) # linear layer \n",
    "        )\n",
    "        \n",
    "        # define time embedding projection\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(dim_t, dim_t), # linear layer \n",
    "            nn.SiLU(), # silu activation\n",
    "            nn.Linear(dim_t, dim_t) # linear layer \n",
    "        )\n",
    "        \n",
    "        # define output data projection\n",
    "        self.head = nn.Linear(hidden_layers[-1], d_in)\n",
    "\n",
    "    # define sinusodial time step embedding\n",
    "    def embed_time(self, timesteps, dim_out, max_period=10000):\n",
    "\n",
    "        # half output dimension\n",
    "        half_dim_out = dim_out // 2\n",
    "\n",
    "        # determine tensor of frequencies\n",
    "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half_dim_out, dtype=torch.float32) / half_dim_out)\n",
    "\n",
    "        # push to compute device\n",
    "        freqs = freqs.to(device=timesteps.device)\n",
    "        \n",
    "        # create timestep vs. frequency grid\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "\n",
    "        # creating the time embedding \n",
    "        time_embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "        # case: odd output dimension\n",
    "        if dim_out % 2:\n",
    "\n",
    "            # append additional dimension\n",
    "            time_embedding = torch.cat([time_embedding, torch.zeros_like(time_embedding[:, :1])], dim=-1)\n",
    "\n",
    "        # return timestep embedding\n",
    "        return time_embedding\n",
    "\n",
    "    # get categorical embeddings\n",
    "    def get_embeddings(self):\n",
    "\n",
    "        # return categorical embeddings\n",
    "        return self.cat_embedding.weight.data\n",
    "\n",
    "    # perform categorical embedding\n",
    "    def embed_categorical(self, x_cat):\n",
    "\n",
    "        # perform categorical embedding\n",
    "        x_cat_emb = self.cat_embedding(x_cat)\n",
    "\n",
    "        # reshape embedding to original input\n",
    "        x_cat_emb = x_cat_emb.view(-1, x_cat_emb.shape[1] * x_cat_emb.shape[2])\n",
    "\n",
    "        # return categorical embedding\n",
    "        return x_cat_emb\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x, timesteps, label=None):\n",
    "        \n",
    "        # init time embeddings\n",
    "        time_emb = self.embed_time(timesteps, self.dim_t)\n",
    "\n",
    "        # embedd time embeddings\n",
    "        time_emb  = self.time_embed(time_emb )\n",
    "        \n",
    "        # case: data classes available\n",
    "        if label is not None:\n",
    "\n",
    "            # determine label embeddings\n",
    "            time_label_emb = time_emb  + self.label_embedding(label)\n",
    "\n",
    "        # run initial projection layer \n",
    "        x = self.projection(x) \n",
    "        \n",
    "        # add time and label embedding \n",
    "        x = x + time_label_emb\n",
    "\n",
    "        # run backbone forward pass\n",
    "        x =  self.backbone(x)\n",
    "\n",
    "        # run projection forward pass\n",
    "        x = self.head(x)\n",
    "\n",
    "        # return forward pass result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the FinDiff model base diffuser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BaseDiffuser network\n",
    "class BaseDiffuser(object):\n",
    "\n",
    "    # define base diffuser network constructor\n",
    "    def __init__(\n",
    "            self, \n",
    "            total_steps=1000, \n",
    "            beta_start=1e-4, \n",
    "            beta_end=0.02, \n",
    "            device='cpu',\n",
    "            scheduler='linear'\n",
    "        ):\n",
    "\n",
    "        # set diffusion steps\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "        # set diffusion start beta\n",
    "        self.beta_start = beta_start\n",
    "\n",
    "        # set diffusion end beta\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        # set compute device\n",
    "        self.device = device\n",
    "\n",
    "        # set noise schedule alphas and betas\n",
    "        self.alphas, self.betas = self.prepare_noise_schedule(scheduler=scheduler)\n",
    "\n",
    "        # set noise schedule alhpa hats\n",
    "        self.alphas_hat = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    # define noise schedule\n",
    "    def prepare_noise_schedule(self, scheduler: str):\n",
    "\n",
    "        # determine noise scheduler scale\n",
    "        scale = 1000 / self.total_steps\n",
    "\n",
    "        # scale beta start\n",
    "        beta_start = scale * self.beta_start\n",
    "\n",
    "        # scale beta end\n",
    "        beta_end = scale * self.beta_end\n",
    "\n",
    "        # case: linear noise scheduler\n",
    "        if scheduler == 'linear':\n",
    "\n",
    "            # determine linear noise schedule betas\n",
    "            betas = torch.linspace(beta_start, beta_end, self.total_steps)\n",
    "\n",
    "            # determine linear noise schedule alphas\n",
    "            alphas = 1.0 - betas\n",
    "\n",
    "        # case: quadratic noise scheduler\n",
    "        elif scheduler == 'quad':\n",
    "\n",
    "            # determine quadratic noise schedule betas\n",
    "            betas = torch.linspace(self.beta_start ** 0.5, self.beta_end ** 0.5, self.total_steps) ** 2\n",
    "\n",
    "            # determine quadratic noise schedule alphas \n",
    "            alphas = 1.0 - betas\n",
    "\n",
    "        # return noise scheduler alphas and betas\n",
    "        return alphas.to(self.device), betas.to(self.device)\n",
    "\n",
    "    # define random timesteps sampler \n",
    "    def sample_random_timesteps(self, n: int):\n",
    "\n",
    "        # sample random timesteps\n",
    "        t = torch.randint(low=1, high=self.total_steps, size=(n,), device=self.device)\n",
    "\n",
    "        # return random timesteps\n",
    "        return t\n",
    "\n",
    "    # define gaussian noise addition\n",
    "    def add_gauss_noise(self, x_num, t):\n",
    "\n",
    "        # determine noise alpha hat\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alphas_hat[t])[:, None]\n",
    "\n",
    "        # determine noise one minius alpha hat \n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alphas_hat[t])[:, None]\n",
    "\n",
    "        # determine numeric noise\n",
    "        noise_num = torch.randn_like(x_num)\n",
    "\n",
    "        # determine x numeric noise\n",
    "        x_noise_num = sqrt_alpha_hat * x_num + sqrt_one_minus_alpha_hat * noise_num\n",
    "\n",
    "        # return x numeric noise and numeric noise\n",
    "        return x_noise_num, noise_num\n",
    "\n",
    "    # define gaussian noise sampling\n",
    "    def p_sample_gauss(self, model_out, z_norm, timesteps):\n",
    "\n",
    "        # determine noise alpha hat\n",
    "        sqrt_alpha_t = torch.sqrt(self.alphas[timesteps])[:, None]\n",
    "\n",
    "        # determine noise betas\n",
    "        betas_t = self.betas[timesteps][:, None]\n",
    "        \n",
    "        # determine noise one minius alpha hat \n",
    "        sqrt_one_minus_alpha_hat_t = torch.sqrt(1 - self.alphas_hat[timesteps])[:, None]\n",
    "        \n",
    "        epsilon_t = torch.sqrt(self.betas[timesteps][:, None])\n",
    "\n",
    "        # determine random noise\n",
    "        random_noise = torch.randn_like(z_norm)\n",
    "        random_noise[timesteps == 0] = 0.0\n",
    "\n",
    "        # determine model mean\n",
    "        model_mean = ((1 / sqrt_alpha_t) * (z_norm - (betas_t * model_out / sqrt_one_minus_alpha_hat_t)))\n",
    "\n",
    "        # determine z norm\n",
    "        z_norm = model_mean + (epsilon_t * random_noise)\n",
    "\n",
    "        # return z norm\n",
    "        return z_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train the FinDiff model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number unique categorical tokens\n",
    "n_cat_tokens = len(np.unique(train[cat_attrs]))\n",
    "\n",
    "# determine total categorical embedding dimension\n",
    "cat_dim = cat_emb_dim * len(cat_attrs)\n",
    "\n",
    "# determine total numerical embedding dimension\n",
    "num_dim = len(num_attrs)\n",
    "\n",
    "# determine total embedding dimension\n",
    "encoded_dim = cat_dim + num_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the FinDiff synthesizer model \n",
    "synthesizer_model = MLPSynthesizer(\n",
    "    d_in=encoded_dim,\n",
    "    hidden_layers=mlp_layers,\n",
    "    activation=activation,\n",
    "    n_cat_tokens=n_cat_tokens,\n",
    "    n_cat_emb=cat_emb_dim,\n",
    "    n_classes=pd.Series(label).nunique(),\n",
    "    embedding_learned=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the FinDiff base diffuser model\n",
    "diffuser_model = BaseDiffuser(\n",
    "    total_steps=diffusion_steps,\n",
    "    beta_start=diffusion_beta_start,\n",
    "    beta_end=diffusion_beta_end,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init optimizer, scheduler and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine synthesizer model parameters\n",
    "parameters = filter(lambda p: p.requires_grad, synthesizer_model.parameters())\n",
    "\n",
    "# init Adam optimizer\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "# init learning rate scheduler\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)\n",
    "\n",
    "# int mean-squared-error loss\n",
    "loss_fnc = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOG 2024-01-10 02:32:32] epoch: 0000, train-loss: 0.89424664:   3%|▎         | 1/30 [00:08<04:15,  8.83s/it]"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "synthesizer_model.train()\n",
    "\n",
    "# init the training progress bar \n",
    "pbar = tqdm(iterable=range(epochs), position=0, leave=True)\n",
    "\n",
    "# iterate over training epochs\n",
    "for epoch in pbar:\n",
    "\n",
    "    base_params = {'epoch': epoch, 'seed': seed, 'mlp_layers': mlp_layers}\n",
    "\n",
    "    # init epoch training batch losses\n",
    "    batch_losses = []\n",
    "\n",
    "    # iterate over epoch batches\n",
    "    for batch_cat, batch_num, batch_y in dataloader:\n",
    "\n",
    "        # sample diffusion timestep\n",
    "        timesteps = diffuser_model.sample_random_timesteps(n=batch_cat.shape[0])\n",
    "\n",
    "        # determine categorical embeddings\n",
    "        batch_cat_emb = synthesizer_model.embed_categorical(x_cat=batch_cat)\n",
    "\n",
    "        # concatenate categorical and numerical embeddings\n",
    "        batch_cat_num = torch.cat((batch_cat_emb, batch_num), dim=1)\n",
    "\n",
    "        # add diffuser gaussian noise\n",
    "        batch_noise_t, noise_t = diffuser_model.add_gauss_noise(x_num=batch_cat_num, t=timesteps)\n",
    "\n",
    "        # conduct synthesizer model forward pass\n",
    "        predicted_noise = synthesizer_model(x=batch_noise_t, timesteps=timesteps, label=batch_y)\n",
    "\n",
    "        # compute training batch loss\n",
    "        batch_loss = loss_fnc(input=noise_t, target=predicted_noise)\n",
    "\n",
    "        # reset model gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # run model backward pass\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # optimize model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # collect training batch losses\n",
    "        batch_losses.append(batch_loss.detach().cpu().numpy())\n",
    "\n",
    "    # determine mean training epoch loss\n",
    "    batch_losses_mean = np.mean(np.array(batch_losses))\n",
    "\n",
    "    # update learning rate scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # collect mean training epoch loss\n",
    "    train_epoch_losses.append(batch_losses_mean)\n",
    "\n",
    "    # prepare and set training epoch progress bar update\n",
    "    now = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    pbar.set_description('[LOG {}] epoch: {}, train-loss: {}'.format(str(now), str(epoch).zfill(4), str(batch_losses_mean)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training loss progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel('[Training Epoch $e_i$]', fontsize=10)\n",
    "ax.set_ylabel('[MSE Error $\\mathcal{L}^{MSE}$]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc='upper right', numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('FinDiff Training Epochs $e_i$ vs. MSE Error $L^{MSE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data using the FinDiff model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init and set sampling parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of generated samples\n",
    "n_samples = None\n",
    "\n",
    "# set number of diffusion steps\n",
    "diffusion_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use FinDiff to generate new data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init samples to be generated\n",
    "samples = torch.randn((len(label_torch), encoded_dim))\n",
    "\n",
    "# init the generation progress bar \n",
    "pbar = tqdm(iterable=reversed(range(0, diffusion_steps)), position=0, leave=True)\n",
    "\n",
    "# iterate over diffusion steps\n",
    "for diffusion_step in pbar:\n",
    "\n",
    "    # prepare and set training epoch progress bar update\n",
    "    now = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    pbar.set_description('[LOG {}] Diffusion Step: {}'.format(str(now), str(diffusion_step).zfill(4)))\n",
    "\n",
    "    # init diffusion timesteps\n",
    "    timesteps = torch.full((len(label_torch),), diffusion_step, dtype=torch.long)\n",
    "\n",
    "    # run synthesizer model forward pass\n",
    "    model_out = synthesizer_model(x=samples.float(), timesteps=timesteps, label=label_torch)\n",
    "\n",
    "    # run diffuser model forward pass\n",
    "    samples = diffuser_model.p_sample_gauss(model_out, samples, timesteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode generated FinDiff samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sample into numeric and categorical parts\n",
    "samples = samples.detach().numpy()\n",
    "samples_num = samples[:, cat_dim:]\n",
    "samples_cat = samples[:, :cat_dim]\n",
    "\n",
    "# denormalize numeric attributes\n",
    "z_norm_upscaled = num_scaler.inverse_transform(samples_num)\n",
    "z_norm_df = pd.DataFrame(z_norm_upscaled, columns=num_attrs)\n",
    "\n",
    "# get embedding lookup matrix\n",
    "embedding_lookup = synthesizer_model.get_embeddings().cpu()\n",
    "\n",
    "# reshape back to batch_size * n_dim_cat * cat_emb_dim\n",
    "samples_cat = samples_cat.reshape(-1, len(cat_attrs), cat_emb_dim)\n",
    "\n",
    "# compute pairwise distances\n",
    "distances = torch.cdist(x1=embedding_lookup, x2=torch.Tensor(samples_cat))\n",
    "\n",
    "# get the closest distance based on the embeddings that belong to a column category\n",
    "z_cat_df = pd.DataFrame(index=range(len(samples_cat)), columns=cat_attrs)\n",
    "\n",
    "nearest_dist_df = pd.DataFrame(index=range(len(samples_cat)), columns=cat_attrs)\n",
    "\n",
    "# iterate over categorical attributes\n",
    "for attr_idx, attr_name in enumerate(cat_attrs):\n",
    "\n",
    "    attr_emb_idx = list(vocab_per_attr[attr_name])\n",
    "    attr_distances = distances[:, attr_emb_idx, attr_idx]\n",
    "\n",
    "    nearest_values, nearest_idx = torch.min(attr_distances, dim=1)\n",
    "    nearest_idx = nearest_idx.cpu().numpy()\n",
    "\n",
    "    z_cat_df[attr_name] = np.array(attr_emb_idx)[nearest_idx]  # need to map emb indices back to column indices\n",
    "    nearest_dist_df[attr_name] = nearest_values.cpu().numpy()\n",
    "\n",
    "z_cat_df = z_cat_df.apply(label_encoder.inverse_transform)\n",
    "\n",
    "samples_decoded = pd.concat([z_cat_df, z_norm_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a metadata for evaluation (from SDV)\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data=train)\n",
    "\n",
    "# generate quality report\n",
    "quality_report = sdv_st.evaluate_quality(\n",
    "    real_data=train,\n",
    "    synthetic_data=samples_decoded,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Column Shapes -> referred to the \"Fidelity Column\" in the paper\n",
    "fig = quality_report.get_visualization(property_name='Column Shapes')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Column Pair Trends -> referred to the \"Fidelity Row\" in the paper\n",
    "fig = quality_report.get_visualization(property_name='Column Pair Trends')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
